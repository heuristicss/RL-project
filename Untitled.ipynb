{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "\n",
    "#%autoreload 1\n",
    "\n",
    "#%aimport gridworld\n",
    "import gridworld\n",
    "import mdp\n",
    "import util\n",
    "from qlearningAgents import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PacmanQAgent in module qlearningAgents:\n",
      "\n",
      "class PacmanQAgent(QLearningAgent)\n",
      " |  PacmanQAgent(epsilon=0.05, gamma=0.8, alpha=0.2, numTraining=0, **args)\n",
      " |  \n",
      " |  Exactly the same as QLearningAgent, but with different default parameters\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PacmanQAgent\n",
      " |      QLearningAgent\n",
      " |      learningAgents.ReinforcementAgent\n",
      " |      learningAgents.ValueEstimationAgent\n",
      " |      game.Agent\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, epsilon=0.05, gamma=0.8, alpha=0.2, numTraining=0, **args)\n",
      " |      These default parameters can be changed from the pacman.py command line.\n",
      " |      For example, to change the exploration rate, try:\n",
      " |          python pacman.py -p PacmanQLearningAgent -a epsilon=0.1\n",
      " |      \n",
      " |      alpha    - learning rate\n",
      " |      epsilon  - exploration rate\n",
      " |      gamma    - discount factor\n",
      " |      numTraining - number of training episodes, i.e. no learning after these many episodes\n",
      " |  \n",
      " |  getAction(self, state)\n",
      " |      Simply calls the getAction method of QLearningAgent and then\n",
      " |      informs parent of action for Pacman.  Do not change or remove this\n",
      " |      method.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from QLearningAgent:\n",
      " |  \n",
      " |  computeActionFromQValues(self, state)\n",
      " |      Compute the best action to take in a state.  Note that if there\n",
      " |      are no legal actions, which is the case at the terminal state,\n",
      " |      you should return None.\n",
      " |  \n",
      " |  computeValueFromQValues(self, state)\n",
      " |      Returns max_action Q(state,action)\n",
      " |      where the max is over legal actions.  Note that if\n",
      " |      there are no legal actions, which is the case at the\n",
      " |      terminal state, you should return a value of 0.0.\n",
      " |  \n",
      " |  getPolicy(self, state)\n",
      " |      What is the best action to take in the state. Note that because\n",
      " |      we might want to explore, this might not coincide with getAction\n",
      " |      Concretely, this is given by\n",
      " |      \n",
      " |      policy(s) = arg_max_{a in actions} Q(s,a)\n",
      " |      \n",
      " |      If many actions achieve the maximal Q-value,\n",
      " |      it doesn't matter which is selected.\n",
      " |  \n",
      " |  getQValue(self, state, action)\n",
      " |      Returns Q(state,action)\n",
      " |      Should return 0.0 if we have never seen a state\n",
      " |      or the Q node value otherwise\n",
      " |  \n",
      " |  getValue(self, state)\n",
      " |      What is the value of this state under the best action?\n",
      " |      Concretely, this is given by\n",
      " |      \n",
      " |      V(s) = max_{a in actions} Q(s,a)\n",
      " |  \n",
      " |  update(self, state, action, nextState, reward)\n",
      " |      The parent class calls this to observe a\n",
      " |      state = action => nextState and reward transition.\n",
      " |      You should do your Q-Value update here\n",
      " |      \n",
      " |      NOTE: You should never call this function,\n",
      " |      it will be called on your behalf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from learningAgents.ReinforcementAgent:\n",
      " |  \n",
      " |  doAction(self, state, action)\n",
      " |      Called by inherited class when\n",
      " |      an action is taken in a state\n",
      " |  \n",
      " |  final(self, state)\n",
      " |      Called by Pacman game at the terminal state\n",
      " |  \n",
      " |  getLegalActions(self, state)\n",
      " |      Get the actions available for a given\n",
      " |      state. This is what you should use to\n",
      " |      obtain legal actions for a state\n",
      " |  \n",
      " |  isInTesting(self)\n",
      " |  \n",
      " |  isInTraining(self)\n",
      " |  \n",
      " |  observationFunction(self, state)\n",
      " |      This is where we ended up after our last action.\n",
      " |      The simulation should somehow ensure this is called\n",
      " |  \n",
      " |  observeTransition(self, state, action, nextState, deltaReward)\n",
      " |      Called by environment to inform agent that a transition has\n",
      " |      been observed. This will result in a call to self.update\n",
      " |      on the same arguments\n",
      " |      \n",
      " |      NOTE: Do *not* override or call this function\n",
      " |  \n",
      " |  registerInitialState(self, state)\n",
      " |  \n",
      " |  setDiscount(self, discount)\n",
      " |  \n",
      " |  setEpsilon(self, epsilon)\n",
      " |      ################################\n",
      " |      # Controls needed for Crawler  #\n",
      " |      ################################\n",
      " |  \n",
      " |  setLearningRate(self, alpha)\n",
      " |  \n",
      " |  startEpisode(self)\n",
      " |      Called by environment when new episode is starting\n",
      " |  \n",
      " |  stopEpisode(self)\n",
      " |      Called by environment when episode is done\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from game.Agent:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "env = ApproximateQAgent(extractor='SimpleExtractor')\n",
    "print(help(PacmanQAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n"
     ]
    }
   ],
   "source": [
    "print (env.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pacman.py'; 'pacman' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-80703638648e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpacman\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#def runGames(layout, pacman, ghosts, display, numGames, record, numTraining=0, catchExceptions=False, timeout=30):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__main__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0m__main__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_display'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pacman.py'; 'pacman' is not a package"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
